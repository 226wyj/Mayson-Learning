| LEC# | TITLE                                                       | Reading      | Assignment      |
| ---- | ----------------------------------------------------------- | ------------ | --------------- |
| 1    | The Column Space of A Contains All Vectors Ax               | I.1 ✅        | I.1[1,4,9,18] ✅ |
| 2    | Multiplying and Factoring Matrices                          | I.2 ✅        | I.2[2,6]     ✅  |
| 3    | Orthonormal Columns In Q Give Q’Q= I                        | I.5          |                 |
| 4    | Eigenvalues and Eigenvectors                                | I.6          |                 |
| 5    | Positive Definite and Semidefinite Matrices                 | I.7          |                 |
| 6    | Singular Value Decomposition (SVD)                          | I.8          |                 |
| 7    | Eckart-Young: The Closest Rank k Matrix to A                | I.9          |                 |
| 8    | Norms of Vectors and Matrices                               | I.11         |                 |
| 9    | Four Ways to Solve Least Squares Problems                   | II.2         |                 |
| 10   | Survey of Difficulties with Ax = b                          | Intro Ch.2   |                 |
| 11   | Minimizing ‖x‖ Subject to Ax = b                            | I.11         |                 |
| 12   | Computing Eigenvalues and Singular Values                   | II.1         |                 |
| 13   | Randomized Matrix Manipulation                              | II.4         |                 |
| 14   | Low Rank Changes in A and Its Inverse                       | III.1        |                 |
| 15   | Matrices A(t) depending on t / Derivative = dA/dt           | III.1-2      |                 |
| 16   | Derivatives of Inverse and Singular Values                  | III.1-2      |                 |
| 17   | Rapidly Decreasing Singular Values                          | III.3        |                 |
| 18   | Counting Parameters in SVD, LU, QR, Saddle Points           | III.2        |                 |
| 19   | Saddle Points Continued / Maxmin Principle                  | III.2, V.1   |                 |
| 20   | Definitions and Inequalities                                | V.1, V.3     |                 |
| 21   | Minimizing a Function Step by Step                          | VI.1, VI.4   |                 |
| 22   | Gradient Descent: Downhill to a Minimum                     | VI.4         |                 |
| 23   | Accelerating Gradient Descent (Use Momentum)                | VI.4         |                 |
| 24   | Linear Programming and TwoPerson Games                      | VI.2, VI.3   |                 |
| 25   | Stochastic Gradient Descent                                 | VI.5         |                 |
| 26   | Structure of Neural Nets for Deep Learning                  | VII.1        |                 |
| 27   | Backpropagation to Find Derivative of the Learning Function | VII.2        |                 |
| 28   | Computing in Class                                          | VII.2        |                 |
| 30   | Completing a Rank-One Matrix / Circulants!                  | IV.8,IV.2    |                 |
| 31   | Eigenvectors of Circulant Matrices: Fourier Matrix          | IV.2         |                 |
| 32   | ImageNet is a CNN / The Convolution Rule                    | IV.2         |                 |
| 33   | Neural Nets and the Learning Function                       | VII.1, IV.10 |                 |
| 34   | Distance Matrices / Procrustes Problem / First Project      | IV.9, IV.10  |                 |
| 35   | Finding Clusters in Graphs / Second Project: Handwriting    | IV.6, IV.7   |                 |